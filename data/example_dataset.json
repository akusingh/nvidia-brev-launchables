[
  {
    "instruction": "Explain what a GPU is in simple terms.",
    "input": "",
    "output": "A GPU (Graphics Processing Unit) is a specialized electronic chip designed to quickly process and render images and graphics. While originally made for gaming and video applications, GPUs are now widely used for AI and machine learning because they can perform many calculations simultaneously, making them much faster than traditional CPUs for certain tasks."
  },
  {
    "instruction": "Write a Python function to calculate factorial.",
    "input": "",
    "output": "Here's a simple Python function to calculate factorial:\n\n```python\ndef factorial(n):\n    if n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\n# Example usage\nprint(factorial(5))  # Output: 120\n```\n\nThis uses recursion to multiply the number by the factorial of the previous number until reaching the base case."
  },
  {
    "instruction": "What are the benefits of fine-tuning LLMs?",
    "input": "",
    "output": "Fine-tuning Large Language Models offers several benefits:\n\n1. **Domain Adaptation**: Customize the model for specific industries or use cases\n2. **Improved Accuracy**: Better performance on your specific tasks compared to generic models\n3. **Reduced Hallucinations**: More reliable outputs for your domain\n4. **Efficiency**: Smaller fine-tuned models can match or exceed larger generic models\n5. **Cost-Effective**: Parameter-efficient methods like LoRA make it affordable\n6. **Customization**: Adapt the model's tone, style, and knowledge to your needs"
  },
  {
    "instruction": "Compare LoRA and full fine-tuning.",
    "input": "What are the main differences?",
    "output": "Here are the key differences between LoRA and full fine-tuning:\n\n**LoRA (Low-Rank Adaptation):**\n- Only trains small adapter layers (~0.1-1% of parameters)\n- Requires less GPU memory (can train 7B models on 12GB)\n- Faster training and iteration\n- Small checkpoint files (few MB)\n- Easy to switch between adapters\n\n**Full Fine-Tuning:**\n- Trains all model parameters (100%)\n- Requires significant GPU memory (multiple A100s for large models)\n- Slower and more expensive\n- Large checkpoint files (GBs)\n- Can potentially achieve slightly better performance\n\nFor most use cases, LoRA provides 95%+ of the performance at a fraction of the cost and time."
  }
]
